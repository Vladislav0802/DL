{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e3e7b9-2aa1-4793-9f5f-94930461b4f0",
   "metadata": {},
   "source": [
    "# Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054b21f-0115-4d82-b972-de0544d52174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import IPython\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26777daa-90d9-4bf5-a01e-d521652ea7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# versions = {\n",
    "#     \"os\": \"N/A\",  \n",
    "#     \"random\": \"N/A\",\n",
    "#     \"time\": \"N/A\",\n",
    "#     \"collections\": \"N/A\",\n",
    "#     \"numpy\": np.__version__,\n",
    "#     \"pandas\": pd.__version__,\n",
    "#     \"cv2\": cv2.__version__,\n",
    "#     \"Pillow\": Image.__version__,\n",
    "#     \"matplotlib\": matplotlib.__version__,\n",
    "#     \"IPython\": IPython.__version__,\n",
    "#     \"torch\": torch.__version__,\n",
    "#     \"torchvision\": torchvision.__version__,  \n",
    "#     \"albumentations\": A.__version__,\n",
    "#     \"sklearn\": sklearn.__version__,\n",
    "#     \"tqdm\": tqdm.__version__,\n",
    "# }\n",
    "\n",
    "# with open(\"requirements.txt\", \"w\") as f:\n",
    "#     for library, version in versions.items():\n",
    "#         f.write(f\"{library}=={version}\\n\")\n",
    "\n",
    "# from IPython.display import FileLink\n",
    "# FileLink(r'requirements.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3ebe3-d3a2-4c9f-b35f-06a2d863c81d",
   "metadata": {},
   "source": [
    "*Запишем дирекции*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57c6fb-bca6-4313-9ad8-342ff1930bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/vladislav/Documents/DL/dl-2025-competition-1/data/train\"\n",
    "test_dir = \"/Users/vladislav/Documents/DL/dl-2025-competition-1/data/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9c571-bb78-43c4-83f8-a72a777d6b74",
   "metadata": {},
   "source": [
    "# Проанализируем наши данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addafd08-ae96-4d05-be62-f769dd54ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted([\n",
    "    name for name in os.listdir(data_dir) \n",
    "    if os.path.isdir(os.path.join(data_dir, name))\n",
    "])\n",
    "print(\"Классы:\", len(class_names))\n",
    "\n",
    "class_counts = {}\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    images = [\n",
    "        f for f in os.listdir(class_path) \n",
    "        if os.path.isfile(os.path.join(class_path, f))\n",
    "    ]\n",
    "    class_counts[class_name] = len(images)\n",
    "    \n",
    "#Посмотрим на распределение\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.title(\"Баланс классов\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02502c4-c978-435e-ab62-f81141fb5bbd",
   "metadata": {},
   "source": [
    "*Выведем некоторые случайные картинки*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d48be-8d98-464a-bc78-291ef5359fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_samples(data_dir, num_samples=10):\n",
    "    class_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(num_samples):\n",
    "        class_name = random.choice(class_names)\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "        if not images:\n",
    "            continue\n",
    "            \n",
    "        random_image = random.choice(images)\n",
    "        img = Image.open(os.path.join(class_path, random_image))\n",
    "        \n",
    "        #Отрисовываем\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Class: {class_name}\\nSize: {img.size}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_random_samples(data_dir, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e24f01-a4f2-4568-afe2-dedaff8cea8a",
   "metadata": {},
   "source": [
    "*Основные выводы:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01c546-d4a0-4dbd-9c62-9a9c325b8f0d",
   "metadata": {},
   "source": [
    "Мы видим 100 классов картинок с большим дисбалансом (в некоторых классах всего 30 изображений, что может сказаться на обучении). Значит нам нужно очень аккуратно делить выборку на тренировочную и валидационную. Также достаточно малое качество изображений, значит аугментацию будем пробовать очень осторожно и оставим только наиболее подходящую"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9a9740-b546-48ac-a815-5de7c191f575",
   "metadata": {},
   "source": [
    "# Перейдем к обучению наших моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e995f-18c7-4ee9-aeea-238b42e5b46f",
   "metadata": {},
   "source": [
    "Создадим кастомный датасет. Будем аккуратно **сортировать** индексы (долго не мог найти ошибку именно в этом месте, оказалось что мы неверно сортировали наши картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3a254-2190-434a-a57f-c979e44610f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = sorted(\n",
    "            [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))],\n",
    "            key=lambda x: int(x) \n",
    "        )\n",
    "\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.idx_to_class = {i: cls for i, cls in enumerate(self.classes)}\n",
    "        \n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(data_dir, class_name)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append((\n",
    "                        os.path.join(class_path, img_name),\n",
    "                        self.class_to_idx[class_name]\n",
    "                    ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83871f4f-3374-4962-94b5-34596e645cc8",
   "metadata": {},
   "source": [
    "*Попробуем разные аугментации*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decff701-f60f-4f2f-9ae0-87821d107ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = A.Compose([\n",
    "#     A.Resize(32, 32),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "#     A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ToTensorV2(),\n",
    "# ])\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, rotate_limit=15), \n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2),            \n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "    ToTensorV2(),  \n",
    "])    \n",
    "val_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b1c56-63f9-4b8d-a797-0d08c065c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data_dir, transform=train_transform)\n",
    "val_dataset = CustomDataset(data_dir, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbfce4-bd6c-498d-9dbe-058fd7e494ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(data_dir, batch_size=32):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03045c8d-1f49-47fc-8b5e-93f315580efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_loaders(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1be7b-2f44-4265-8629-7ee66c31f848",
   "metadata": {},
   "source": [
    "*Посмотрим на аугментированные картинки*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62dd674-af91-44d0-a5cf-3639469ee1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_augmented_samples(dataloader, num_samples=8):\n",
    "    images, labels = next(iter(dataloader))\n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    images = images.permute(0, 2, 3, 1).numpy()  \n",
    "    images = std * images + mean  \n",
    "    images = np.clip(images, 0, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "    for i in range(num_samples):\n",
    "        axes[i].imshow(images[i])\n",
    "        axes[i].set_title(f\"Class: {labels[i].item()}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "show_augmented_samples(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a642f-a87b-4840-bbb2-4305675ca2c9",
   "metadata": {},
   "source": [
    "Получилось достаточно неплохо, наши аугментации не сильно испортили картинки, значит можем идти дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb2f04f-3cd6-487c-8697-f712b90cce6f",
   "metadata": {},
   "source": [
    "*Разделим нашу выборку на трейн-валидацию, учитывая дисбаланс*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb9709-e24f-49d7-8bdd-d4a2fc3f2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedSubset(Subset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.transform = transform\n",
    "        self.idx_to_class = dataset.idx_to_class \n",
    "        self.class_to_idx = dataset.class_to_idx\n",
    "        self.classes = dataset.classes\n",
    "        self.samples = [dataset.samples[i] for i in indices]  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[self.indices[idx]]\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "def get_stratified_split(dataset, val_size=0.2):\n",
    "    labels = [label for _, label in dataset.samples]\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        range(len(dataset)),\n",
    "        test_size=val_size,\n",
    "        stratify=labels\n",
    "    )\n",
    "    return train_idx, val_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fcee0-e06e-409d-80d3-deafb4c6b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CustomDataset(data_dir, transform=None)\n",
    "train_idx, val_idx = get_stratified_split(full_dataset)\n",
    "\n",
    "train_dataset = StratifiedSubset(full_dataset, train_idx, train_transform)\n",
    "val_dataset = StratifiedSubset(full_dataset, val_idx, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d591720-ae05-40f3-bad2-89d18608ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter          # здесь можно посмотреть на распределение выборки\n",
    "\n",
    "# def check_split(dataset, name):\n",
    "#     labels = [dataset[i][1] for i in range(len(dataset))]\n",
    "#     print(f\"{name} распределение:\", Counter(labels))\n",
    "# check_split(train_dataset, \"Train\")\n",
    "# check_split(val_dataset, \"Val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d4c31-44f0-4bcd-909c-f3ba88007077",
   "metadata": {},
   "source": [
    "# Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73abc5-dbfe-4ae4-b14e-d35df8fc9867",
   "metadata": {},
   "source": [
    "Сперва создадим различные функции, которые пригодятся нам в обучении. Затем будем пробовать разные модельки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3134c65-452a-4532-92b3-d7854d21e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, mode='min', patience=5, factor=0.1):\n",
    "\n",
    "    return ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode=mode, \n",
    "        patience=patience, \n",
    "        factor=factor,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f855f6e7-3803-4d87-824d-38b6a4a41c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_warmup(optimizer, current_epoch, warmup_epochs=5, base_lr=0.001):\n",
    "   \n",
    "    if current_epoch < warmup_epochs:\n",
    "        lr = base_lr * (current_epoch + 1) / warmup_epochs\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ccdbb2-4da8-4fa3-bb89-4ea8861b7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, delta=0, mode='min'):\n",
    "       \n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_metric = float('inf') if mode == 'min' else -float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_metric):\n",
    "        if self.mode == 'min':\n",
    "            improved = current_metric < (self.best_metric - self.delta)\n",
    "        else:\n",
    "            improved = current_metric > (self.best_metric + self.delta)\n",
    "\n",
    "        if improved:\n",
    "            self.best_metric = current_metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33762168-761a-403e-b84a-c373bc1cd18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b26f3be-47b4-462b-ae60-e4dd09ac476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, live_update=True):\n",
    "\n",
    "    if not hasattr(plot_training_history, 'fig') or not live_update:\n",
    "        plot_training_history.fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        plt.ion()  \n",
    "    else:\n",
    "        ax1, ax2 = plot_training_history.fig.axes\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "\n",
    "    ax1.plot(history['train_loss'], label='Train', color='blue')\n",
    "    ax1.plot(history['val_loss'], label='Validation', color='orange')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(history['train_acc'], label='Train', color='blue')\n",
    "    ax2.plot(history['val_acc'], label='Validation', color='orange')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    last_epoch = len(history['train_loss']) - 1\n",
    "    print(f\"Epoch {last_epoch + 1}: \"\n",
    "          f\"Train Loss = {history['train_loss'][-1]:.4f}, \"\n",
    "          f\"Val Loss = {history['val_loss'][-1]:.4f} | \"\n",
    "          f\"Train Acc = {history['train_acc'][-1]:.2f}%, \"\n",
    "          f\"Val Acc = {history['val_acc'][-1]:.2f}%\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if live_update:\n",
    "        display.clear_output(wait=True)  \n",
    "        display.display(plot_training_history.fig) \n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f3fd0-a4e2-42ea-9511-fbb5641d9eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add92da-982f-44bd-979a-3585cc6bb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_logging(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: str = \"cuda\",\n",
    "    num_epochs: int = 20,\n",
    "    warmup_epochs: int = 3,\n",
    "    patience: int = 5,\n",
    "    model_save_path: str = 'best_model.pth' \n",
    ") -> dict:\n",
    "\n",
    "    model.to(device)\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'train_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0 \n",
    "    best_epoch = 0\n",
    "    \n",
    "    scheduler = get_scheduler(optimizer, mode='min', patience=3)\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "\n",
    "    plt.ion()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    metrics_text = \"\"\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    " \n",
    "        apply_warmup(optimizer, epoch, warmup_epochs)\n",
    "     \n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device).requires_grad_(), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "     \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'train_acc': 100 * correct_train / total_train,\n",
    "            }, model_save_path)\n",
    "            print(f\"Новая лучшая модель сохранена с val_acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        epoch_info = (\n",
    "            f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.2f}% | \"\n",
    "            f\"Val Acc: {val_acc:.2f}% | \"\n",
    "            f\"Time: {epoch_time:.2f}s\\n\"\n",
    "        )\n",
    "        \n",
    "        metrics_text += epoch_info\n",
    "       \n",
    "        ax1.clear()\n",
    "        ax1.plot(history['train_loss'], label='Train', color='blue')\n",
    "        ax1.plot(history['val_loss'], label='Validation', color='orange')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.clear()\n",
    "        ax2.plot(history['train_acc'], label='Train', color='blue')\n",
    "        ax2.plot(history['val_acc'], label='Validation', color='orange')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.legend()\n",
    "        \n",
    "        display.clear_output(wait=True)  \n",
    "        print(metrics_text)  \n",
    "        display.display(fig)  \n",
    "        \n",
    "        plt.pause(0.1)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "   \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nЛучшая модель достигнута на эпохе {best_epoch+1} с val_acc: {best_val_acc:.2f}%\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c9ec4-3f58-4aa6-bc92-a7f616773463",
   "metadata": {},
   "source": [
    "# Простая моделька (без Transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f473a3-8509-4999-af27-5bd5adb1a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )   \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64 * 28 * 28, 256), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model_simple = SimpleCNN(num_classes=100).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# model_simple.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0577657-ccdc-44b2-ac9f-e380aea2fe3d",
   "metadata": {},
   "source": [
    "*Обучение!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3fd8a-bad5-4e83-891d-2369944d4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = model_simple.to(device)\n",
    "optimizer = optim.Adam(model_simple.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = train_loader\n",
    "val_loader = val_loader\n",
    "\n",
    "history_simple = train_model_with_logging(\n",
    "    model=model_simple,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=20,\n",
    "    warmup_epochs=3,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39f805-e2fa-44e5-a777-641fc689f22c",
   "metadata": {},
   "source": [
    "# EfficientNet_B3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7e91d-31f7-4c59-b9ff-6a1027670b36",
   "metadata": {},
   "source": [
    "Будем использовать эту модель. Она справилась лучше **ResNet-18**, но требует больше времени для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeea907-9ecc-4486-af4a-884cfed86f3f",
   "metadata": {},
   "source": [
    "*Подгрузим модель*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ad6ca-d8b5-4bdc-abd1-f8bee26994f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b0e56-0712-442e-81d8-6180ce8da60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():       #Можно посмотреть параметры модели, какие слои где находятся\n",
    "#      print(name, param.requires_grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75151ebf-744b-400a-bfb0-75b1b04b9952",
   "metadata": {},
   "source": [
    "*Обучение!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ba0c2-3535-4cf4-b0e5-8f13a76a2286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = train_loader\n",
    "val_loader = val_loader\n",
    "\n",
    "history = train_model_with_logging(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=20,\n",
    "    warmup_epochs=3,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91297ebc-8223-45b0-8fea-f5afacbabc3c",
   "metadata": {},
   "source": [
    "Кажется модель обучилась достаточно неплохо. Можно попробовать увеличить количество эпох. Примечательно, что трейн и валидация сходятся, а значит мы все делаем правильно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2a156-9c2e-41c8-aa75-1180430a8a73",
   "metadata": {},
   "source": [
    "# Предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104b820-0d9e-48a4-a038-7eb04cb2e13e",
   "metadata": {},
   "source": [
    "*Запишем предсказания в файл. Не забудем создать трансформ: нормализуем и делаем ресайз как в обучении!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42704364-6ef8-4e73-a8e9-4344ab6b197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "file_list = sorted(\n",
    "    [f for f in os.listdir(test_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n",
    "    key=lambda x: int(x.split('.')[0]) \n",
    ")\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': [int(f.split('.')[0]) for f in file_list],\n",
    "    'file_path': [os.path.join(test_dir, f) for f in file_list],\n",
    "    'target': 0  \n",
    "})\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for idx, row in tqdm(submission.iterrows(), total=len(submission)):\n",
    "    image = cv2.imread(row['file_path'])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    augmented = test_transform(image=image)\n",
    "    image_tensor = augmented[\"image\"].unsqueeze(0).to(device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output.argmax().item()\n",
    "    \n",
    "    predictions.append(pred)\n",
    "\n",
    "submission['target'] = predictions\n",
    "submission = submission.sort_values('id').drop('file_path', axis=1)\n",
    "submission.to_csv('submission_newly.csv', index=False)\n",
    "\n",
    "print(\"Первые 5 предсказаний:\")\n",
    "print(submission.head())\n",
    "print(\"\\nСтатистика предсказаний:\")\n",
    "print(submission['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b518d-598f-40be-924d-59d1031f5daf",
   "metadata": {},
   "source": [
    "# Генератор"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d2d797-9044-4c2c-b837-be9ae28fff5f",
   "metadata": {},
   "source": [
    "Создадим простой генератор (типа GAN): начнем с реализации генератора и дискриминатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496cb290-ded3-474b-9207-30cfbae83f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_size = 224  \n",
    "batch_size = 128\n",
    "num_classes = 100\n",
    "latent_dim = 100\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(data_dir, transform=transform)\n",
    "dataloader, loader= get_loaders(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5408955-6e41-4685-a2a0-1f300caab6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, num_classes=100, img_size=224):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        input_dim = latent_dim + num_classes\n",
    "        self.init_size = img_size // 16  # 224 -> 14\n",
    "\n",
    "        self.l1 = nn.Sequential(nn.Linear(input_dim, 256 * self.init_size * self.init_size))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Upsample(scale_factor=2),  # 14 → 28\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),  # 28 → 56\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),  # 56 → 112\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),  # 112 → 224\n",
    "            nn.Conv2d(64, 3, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        x = torch.cat((noise, self.label_emb(labels)), dim=1)\n",
    "        out = self.l1(x)\n",
    "        out = out.view(out.size(0), 256, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f70c25-42df-4fb6-97e4-8a0c6ee98cea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=100, img_size=224):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.label_embedding = nn.Embedding(num_classes, 1 * img_size * img_size)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
    "                     nn.LeakyReLU(0.2, inplace=True),\n",
    "                     nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(4, 64, bn=False),   \n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "        )\n",
    "\n",
    "        ds_size = img_size // 2 ** 4  # 224 / 16 = 14\n",
    "        self.adv_layer = nn.Sequential(nn.Flatten(), nn.Linear(512 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        class_map = self.label_embedding(labels).view(labels.size(0), 1, self.img_size, self.img_size)\n",
    "        d_in = torch.cat((img, class_map), 1)\n",
    "        out = self.model(d_in)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089d455-8a34-4c99-aa9f-ff4287ee3d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim=latent_dim, num_classes=num_classes, img_size=img_size).to(device)\n",
    "discriminator = Discriminator(num_classes=num_classes, img_size=img_size).to(device)\n",
    "\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c90518-2810-4e6f-a8f1-00d26f3cea13",
   "metadata": {},
   "source": [
    "*Запустим обучение и будем следить за изменением лосса на генераторе и дискриминаторе*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91beffb6-2790-4e02-bcd6-a2f1bfeca121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "sample_interval = 5 \n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (real_imgs, labels) in enumerate(dataloader):  \n",
    "        batch_size = real_imgs.size(0)\n",
    "        real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
    "\n",
    "        valid = torch.ones((batch_size, 1), device=device)\n",
    "        fake = torch.zeros((batch_size, 1), device=device)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        z = torch.randn(batch_size, latent_dim, device=device)\n",
    "        gen_labels = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs, gen_labels), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "       \n",
    "        for _ in range(5):  \n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs, labels), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), gen_labels), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(dataloader)}] \"\n",
    "                  f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "    if epoch % sample_interval == 0:\n",
    "        z = torch.randn(10, latent_dim, device=device)\n",
    "        labels_sample = torch.arange(10, device=device)  \n",
    "        gen_imgs = generator(z, labels_sample)\n",
    "        gen_imgs = (gen_imgs + 1) / 2  \n",
    "        save_image(gen_imgs, f\"generated_images/epoch_{epoch}.png\", nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae7081-aa04-4d74-9c22-497446be0864",
   "metadata": {},
   "source": [
    "*Посмотрим на сгенерированные картинки*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8701ebe-3718-4085-bd42-8c6e4522847f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T07:39:20.915513Z",
     "iopub.status.busy": "2025-04-30T07:39:20.914538Z",
     "iopub.status.idle": "2025-04-30T07:39:21.178879Z",
     "shell.execute_reply": "2025-04-30T07:39:21.177905Z",
     "shell.execute_reply.started": "2025-04-30T07:39:20.915473Z"
    },
    "tags": []
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2176e29-cb34-4424-8fd2-7e56b82c63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(generator, z_dim, device, n_images=15, images_per_row=5, n_classes=100):\n",
    "    generator.eval()\n",
    "    z = torch.randn(n_images, z_dim).to(device)\n",
    "    random_labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gen_imgs = generator(z, random_labels).cpu()\n",
    "\n",
    "    fig, axs = plt.subplots(n_images // images_per_row, images_per_row, figsize=(images_per_row * 4, (n_images // images_per_row) * 4))  \n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i, img in enumerate(gen_imgs):\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        axs[i].imshow((img_np - img_np.min()) / (img_np.max() - img_np.min())) \n",
    "        axs[i].set_title(f\"Class {random_labels[i].item()}\")\n",
    "        axs[i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_images(generator, z_dim=100, device=device, n_images=15, images_per_row=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9593a4-ee9d-4c1d-9e58-eed8fd0a6860",
   "metadata": {},
   "source": [
    "# Пайплайн с лучшим скором для Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec503d9-4e4b-4c72-9864-680b82327910",
   "metadata": {},
   "source": [
    "## Напомним необходимые функции и классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab9ed1-1620-49ea-9afa-b01ba4afc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b2966-6add-4291-ae1f-120608acfad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/jupyter/work/resources/data/train\"\n",
    "test_dir = \"/home/jupyter/work/resources/data/test\"\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(\n",
    "            [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))],\n",
    "            key=lambda x: int(x) \n",
    "        )\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.idx_to_class = {i: cls for i, cls in enumerate(self.classes)}\n",
    "        \n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(data_dir, class_name)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append((\n",
    "                        os.path.join(class_path, img_name),\n",
    "                        self.class_to_idx[class_name]\n",
    "                    ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]          \n",
    "        return image, label\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, rotate_limit=15), \n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2),            \n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "    ToTensorV2(),  \n",
    "])    \n",
    "val_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(data_dir, transform=train_transform)\n",
    "val_dataset = CustomDataset(data_dir, transform=val_transform)\n",
    "\n",
    "def get_loaders(data_dir, batch_size=32):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  \n",
    "        pin_memory=True\n",
    "    )   \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    ) \n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = get_loaders(data_dir)\n",
    "\n",
    "class StratifiedSubset(Subset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.transform = transform\n",
    "        self.idx_to_class = dataset.idx_to_class \n",
    "        self.class_to_idx = dataset.class_to_idx\n",
    "        self.classes = dataset.classes\n",
    "        self.samples = [dataset.samples[i] for i in indices]  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[self.indices[idx]]        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]    \n",
    "        return image, label\n",
    "\n",
    "def get_stratified_split(dataset, val_size=0.2):\n",
    "    labels = [label for _, label in dataset.samples]\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        range(len(dataset)),\n",
    "        test_size=val_size,\n",
    "        stratify=labels\n",
    "    )\n",
    "    return train_idx, val_idx\n",
    "\n",
    "full_dataset = CustomDataset(data_dir, transform=None)\n",
    "train_idx, val_idx = get_stratified_split(full_dataset)\n",
    "\n",
    "train_dataset = StratifiedSubset(full_dataset, train_idx, train_transform)\n",
    "val_dataset = StratifiedSubset(full_dataset, val_idx, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d51c92-ffc5-4cbb-9b34-d8c430014fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, mode='min', patience=5, factor=0.1):\n",
    "    return ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode=mode, \n",
    "        patience=patience, \n",
    "        factor=factor,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "def apply_warmup(optimizer, current_epoch, warmup_epochs=5, base_lr=0.001): \n",
    "    if current_epoch < warmup_epochs:\n",
    "        lr = base_lr * (current_epoch + 1) / warmup_epochs\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, delta=0, mode='min'):    \n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_metric = float('inf') if mode == 'min' else -float('inf')\n",
    "        self.early_stop = False\n",
    "    def __call__(self, current_metric):\n",
    "        if self.mode == 'min':\n",
    "            improved = current_metric < (self.best_metric - self.delta)\n",
    "        else:\n",
    "            improved = current_metric > (self.best_metric + self.delta)\n",
    "        if improved:\n",
    "            self.best_metric = current_metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "    \n",
    "device = get_device()\n",
    "\n",
    "def plot_training_history(history, live_update=True):\n",
    "    if not hasattr(plot_training_history, 'fig') or not live_update:\n",
    "        plot_training_history.fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        plt.ion()  \n",
    "    else:\n",
    "        ax1, ax2 = plot_training_history.fig.axes   \n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    ax1.plot(history['train_loss'], label='Train', color='blue')\n",
    "    ax1.plot(history['val_loss'], label='Validation', color='orange')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    ax2.plot(history['train_acc'], label='Train', color='blue')\n",
    "    ax2.plot(history['val_acc'], label='Validation', color='orange')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    last_epoch = len(history['train_loss']) - 1\n",
    "    print(f\"Epoch {last_epoch + 1}: \"\n",
    "          f\"Train Loss = {history['train_loss'][-1]:.4f}, \"\n",
    "          f\"Val Loss = {history['val_loss'][-1]:.4f} | \"\n",
    "          f\"Train Acc = {history['train_acc'][-1]:.2f}%, \"\n",
    "          f\"Val Acc = {history['val_acc'][-1]:.2f}%\")  \n",
    "    plt.tight_layout()   \n",
    "    if live_update:\n",
    "        display.clear_output(wait=True)  \n",
    "        display.display(plot_training_history.fig) \n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train_model_with_logging(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: str = \"cuda\",\n",
    "    num_epochs: int = 20,\n",
    "    warmup_epochs: int = 3,\n",
    "    patience: int = 5,\n",
    "    model_save_path: str = 'best_model.pth' \n",
    ") -> dict:\n",
    "    model.to(device)\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'train_acc': []\n",
    "    }   \n",
    "    best_val_acc = 0.0 \n",
    "    best_epoch = 0\n",
    "    \n",
    "    scheduler = get_scheduler(optimizer, mode='min', patience=3)\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "\n",
    "    plt.ion()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    metrics_text = \"\"\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    " \n",
    "        apply_warmup(optimizer, epoch, warmup_epochs)\n",
    "     \n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device).requires_grad_(), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "     \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'train_acc': 100 * correct_train / total_train,\n",
    "            }, model_save_path)\n",
    "            print(f\"Новая лучшая модель сохранена с val_acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        epoch_info = (\n",
    "            f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.2f}% | \"\n",
    "            f\"Val Acc: {val_acc:.2f}% | \"\n",
    "            f\"Time: {epoch_time:.2f}s\\n\"\n",
    "        )\n",
    "        \n",
    "        metrics_text += epoch_info\n",
    "       \n",
    "        ax1.clear()\n",
    "        ax1.plot(history['train_loss'], label='Train', color='blue')\n",
    "        ax1.plot(history['val_loss'], label='Validation', color='orange')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.clear()\n",
    "        ax2.plot(history['train_acc'], label='Train', color='blue')\n",
    "        ax2.plot(history['val_acc'], label='Validation', color='orange')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.legend()\n",
    "        \n",
    "        display.clear_output(wait=True)  \n",
    "        print(metrics_text)  \n",
    "        display.display(fig)  \n",
    "        \n",
    "        plt.pause(0.1)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()      \n",
    "    plt.ioff()\n",
    "    plt.show() \n",
    "    print(f\"\\nЛучшая модель достигнута на эпохе {best_epoch+1} с val_acc: {best_val_acc:.2f}%\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf28dba-3e37-4b5e-8481-00e428ed8d1e",
   "metadata": {},
   "source": [
    "## Само обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2974a72-8774-4da7-8fad-772e0a5b0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = train_loader\n",
    "val_loader = val_loader\n",
    "\n",
    "history = train_model_with_logging(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=20,\n",
    "    warmup_epochs=3,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4748da2-66f8-4fcb-bda9-0e7cf96d1a89",
   "metadata": {},
   "source": [
    "## Предсказания (если необходимо)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0d678-a177-422d-aee0-b4d82f2aed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "file_list = sorted(\n",
    "    [f for f in os.listdir(test_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n",
    "    key=lambda x: int(x.split('.')[0]) \n",
    ")\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': [int(f.split('.')[0]) for f in file_list],\n",
    "    'file_path': [os.path.join(test_dir, f) for f in file_list],\n",
    "    'target': 0  \n",
    "})\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for idx, row in tqdm(submission.iterrows(), total=len(submission)):\n",
    "    image = cv2.imread(row['file_path'])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    augmented = test_transform(image=image)\n",
    "    image_tensor = augmented[\"image\"].unsqueeze(0).to(device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output.argmax().item()\n",
    "    \n",
    "    predictions.append(pred)\n",
    "\n",
    "submission['target'] = predictions\n",
    "submission = submission.sort_values('id').drop('file_path', axis=1)\n",
    "submission.to_csv('submission_newly.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
